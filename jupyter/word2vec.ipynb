{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引き数処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='data_w2v/', help='Data set directory.')\n",
    "parser.add_argument('--log_dir', type=str, default='logs_w2v/', help='Log directory.')\n",
    "parser.add_argument('--max_vocab', type=int, default=2000, help='Max Vocablary size.')\n",
    "parser.add_argument('--skip_window', type=int, default=2, help='How many words to consider left and right.')\n",
    "parser.add_argument('--num_skips', type=int, default=4, help='How many times to reuse an input to generate a label.')\n",
    "parser.add_argument('--embedding_size', type=int, default=64, help=\"Dimension of the embedding vector.\")\n",
    "parser.add_argument('--num_sumpled', type=int, default=64, help=\"Number of negative examples to sample.\" )\n",
    "parser.add_argument('--num_step', type=int, default=10000, help=\"Train step.\" )\n",
    "parser.add_argument('--batch_size', type=int, default=64, help=\"Batch size.\" )\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help=\"Learning rate.\" )\n",
    "parser.add_argument('--create_tsv', type=bool, default=True, help=\"Create words.tsv or not.\" )\n",
    "\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSetクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, data_dir, max_vocab):\n",
    "\n",
    "        #全データセットのファイルパスを取得\n",
    "        file_pathes = []\n",
    "        for file_path in glob.glob(data_dir+'*'):\n",
    "            file_pathes.append(file_path)\n",
    "\n",
    "        #ファイルを読み込み\n",
    "        row_documents = [self._read_docment(file_path) for file_path in file_pathes]\n",
    "        #必要な部分だけ抽出\n",
    "        documents = [self._preprocessing(document) for document in row_documents]\n",
    "        #形態素解析\n",
    "        splited_documents = [self._morphological(document) for document in documents]\n",
    "\n",
    "        words = []\n",
    "        for word_list in splited_documents:\n",
    "            words.extend(word_list)\n",
    "        \n",
    "        #データセット作成\n",
    "        self.id_sequence, self.word_frequency, self.w_to_id, self.id_to_w = self._build_data_sets(words, max_vocab)\n",
    "        print('Most common words (+UNK)', self.word_frequency[:5])\n",
    "        print('Sample data.')\n",
    "        print(self.id_sequence[:10])\n",
    "        print([self.id_to_w[i] for i in self.id_sequence[:10]])\n",
    "        self.data_index = 0\n",
    "\n",
    "\n",
    "    #ファイルの読み込み\n",
    "    def _read_docment(self, file_path):\n",
    "        with open(file_path, 'r', encoding='sjis') as f:\n",
    "            return f.read()\n",
    "\n",
    "    #ヘッダなどの不要データを前処理。必要な部分だけを返す。\n",
    "    def _preprocessing(self, document):\n",
    "\n",
    "        lines = document.splitlines()\n",
    "        processed_line = []\n",
    "\n",
    "        horizontal_count = 0\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            #ヘッダーは読み飛ばす\n",
    "            if horizontal_count < 2:\n",
    "                if line.startswith('-------'):\n",
    "                    horizontal_count += 1\n",
    "                continue\n",
    "            #フッターに入る行になったらそれ以降は無視\n",
    "            if line.startswith('底本：'):\n",
    "                break\n",
    "\n",
    "            line = re.sub(r'《.*》', '', line) #ルビを除去\n",
    "            line = re.sub(r'［.*］', '', line) #脚注を除去\n",
    "            line =re.sub(r'[!-~]', '', line) #半角記号を除去\n",
    "            line =re.sub(r'[︰-＠]', '', line) #全角記号を除去\n",
    "            line = re.sub('｜', '', line) # 脚注の始まりを除去\n",
    "\n",
    "            processed_line.append(line)\n",
    "\n",
    "        return ''.join(processed_line)\n",
    "\n",
    "    #形態素解析\n",
    "    def _morphological(self, document):\n",
    "\n",
    "        word_list = []\n",
    "        t = Tokenizer()\n",
    "        for token in t.tokenize(document):\n",
    "            #名詞（一般）動詞（自立）、形容詞（自立）以外は除外\n",
    "            if token.part_of_speech.startswith('名詞,一般') and token.base_form != '':\n",
    "                word_list.append(token.base_form)\n",
    "            if token.part_of_speech.startswith('動詞,自立') and token.base_form != '':\n",
    "                word_list.append(token.base_form)\n",
    "            if token.part_of_speech.startswith('形容詞,自立') and token.base_form != '':\n",
    "                word_list.append(token.base_form)\n",
    "        return word_list\n",
    "\n",
    "    #辞書作成\n",
    "    def _build_data_sets(self, words, max_vocab):\n",
    "\n",
    "        #単語出現回数を解析。出現数が少ないたんをUnknown wordとしてひとくくりに扱う\n",
    "        word_frequency = [['UNW', -1]]\n",
    "        word_frequency.extend(collections.Counter(words).most_common(max_vocab - 1))\n",
    "        #単語=>IDの辞書\n",
    "        w_to_id = dict()\n",
    "        for word, _ in word_frequency:\n",
    "            w_to_id[word] = len(w_to_id)\n",
    "        #形態素解析した文章を単語IDの並びに変換\n",
    "        id_sequence = list()\n",
    "        unw_count = 0\n",
    "        for word in words:\n",
    "            #UNK処理\n",
    "            if word in w_to_id:\n",
    "                index = w_to_id[word]\n",
    "            else:\n",
    "                index = 0\n",
    "                unw_count += 1\n",
    "            id_sequence.append(index)\n",
    "        word_frequency[0][1] = unw_count\n",
    "        #単語ID=>単語の辞書\n",
    "        id_to_w = dict(zip(w_to_id.values(), w_to_id.keys()))\n",
    "        return id_sequence, word_frequency, w_to_id, id_to_w\n",
    "\n",
    "\n",
    "    # num_skip:１つの入力をどれだけ再利用するか\n",
    "    # skip_window: 左右何語までを正解対象にするか\n",
    "    def create_next_batch(self, batch_size, num_skips, skip_window):\n",
    "\n",
    "        assert batch_size % num_skips == 0\n",
    "        #一つの入力の再利用回数が対象範囲全件を超えてはならない\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        inputs = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        span = 2 * skip_window + 1\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        #データセットが1週しそうならindexを最初にもどす\n",
    "        if self.data_index + span > len(self.id_sequence):\n",
    "            self.data_index = 0\n",
    "        #初期のqueueを構築(window内の単語をすべて格納)\n",
    "        buffer.extend(self.id_sequence[self.data_index:self.data_index+span])\n",
    "        self.data_index += span\n",
    "\n",
    "        for i in range(batch_size // num_skips):\n",
    "            #中心は先に正解データから除外\n",
    "            target = skip_window\n",
    "            targets_to_avoid = [skip_window]\n",
    "            for j in range(num_skips):\n",
    "                #すでに選ばれている物以外から正解データのインデックスを取得\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                #次回以降targetにならないように\n",
    "                targets_to_avoid.append(target)\n",
    "                #入力値になるのはbufferの中心\n",
    "                inputs[i * num_skips + j] = buffer[skip_window]\n",
    "                #ランダムに指定した周辺単語が正解データに\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "            #次に入れる単語がデータセットにない場合はbufferには最初の値を入力\n",
    "            if self.data_index == len(self.id_sequence):\n",
    "                buffer = self.id_sequence[:span]\n",
    "                self.data_index = span\n",
    "            else:\n",
    "                #bufferに次の単語を追加してindexを1進める\n",
    "                buffer.append(self.id_sequence[self.data_index])\n",
    "                self.data_index += 1\n",
    "        #最後の方のデータが使われないことを避けるために少しだけindexを元に戻す\n",
    "        self.data_index = (self.data_index + len(self.id_sequence) - span) % len(self.id_sequence)\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットオブジェクトを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNW', 0], ('する', 74), ('ゐる', 36), ('つて', 18), ('描く', 17)]\n",
      "Sample data.\n",
      "[147, 148, 149, 78, 21, 150, 17, 46, 22, 47]\n",
      "['始原', '言葉', '従', 'ふむ', '絵画', 'あら', 'はれる', 'ちる', 'ふる', '内部']\n"
     ]
    }
   ],
   "source": [
    "data = DataSet(FLAGS.data_dir, FLAGS.max_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings用に使うラベルをtsv形式で保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings metadata was saved to logs_w2v//words.tsv\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.create_tsv:\n",
    "    sorted_dict = sorted(data.w_to_id.items(), key=lambda x: x[1])\n",
    "    words = [\"{word}\\n\".format(word=x[0]) for x in sorted_dict]\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    with open(FLAGS.log_dir+\"words.tsv\", 'w', encoding=\"utf-8\") as f:\n",
    "        f.writelines(words)\n",
    "    print(\"Embeddings metadata was saved to \"+FLAGS.log_dir+\"/words.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = FLAGS.batch_size\n",
    "embedding_size = FLAGS.embedding_size\n",
    "vocab_size = len(data.w_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# placeholderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中間層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embedding, train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出力層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev =1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases  = tf.Variable(tf.zeros([vocab_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nce_loss = tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, FLAGS.num_sumpled, vocab_size)\n",
    "loss = tf.reduce_mean(nce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_op = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル保存用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初期化 or モデル読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized.\n"
     ]
    }
   ],
   "source": [
    "ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_dir)\n",
    "if ckpt_state:\n",
    "    last_model = ckpt_state.model_checkpoint_path\n",
    "    saver.restore(sess,last_model)\n",
    "    print(\"model was loaded:\", last_model)\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_step = sess.run(global_step)\n",
    "average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  100 :  81.2024235534668\n",
      "Average loss at step  200 :  44.66854076385498\n",
      "Average loss at step  300 :  28.34634283065796\n",
      "Average loss at step  400 :  18.318818426132204\n",
      "Average loss at step  500 :  13.459114770889283\n",
      "Average loss at step  600 :  10.031015138626099\n",
      "Average loss at step  700 :  8.443193678855897\n",
      "Average loss at step  800 :  7.335740728378296\n",
      "Average loss at step  900 :  6.447582573890686\n",
      "Average loss at step  1000 :  5.976786036491394\n",
      "Average loss at step  1100 :  5.711910331249237\n",
      "Average loss at step  1200 :  5.36340083360672\n",
      "Average loss at step  1300 :  5.2000783252716065\n",
      "Average loss at step  1400 :  5.1124384260177616\n",
      "Average loss at step  1500 :  4.988132543563843\n",
      "Average loss at step  1600 :  4.844551739692688\n",
      "Average loss at step  1700 :  4.810387749671936\n",
      "Average loss at step  1800 :  4.755622735023499\n",
      "Average loss at step  1900 :  4.677664103507996\n",
      "Average loss at step  2000 :  4.570290808677673\n",
      "Average loss at step  2100 :  4.528924763202667\n",
      "Average loss at step  2200 :  4.46625082731247\n",
      "Average loss at step  2300 :  4.431032600402832\n",
      "Average loss at step  2400 :  4.426102800369263\n",
      "Average loss at step  2500 :  4.386261429786682\n",
      "Average loss at step  2600 :  4.3150176382064815\n",
      "Average loss at step  2700 :  4.339475746154785\n",
      "Average loss at step  2800 :  4.3002037358284\n",
      "Average loss at step  2900 :  4.234587352275849\n",
      "Average loss at step  3000 :  4.203617112636566\n",
      "Average loss at step  3100 :  4.148779629468918\n",
      "Average loss at step  3200 :  4.131335043907166\n",
      "Average loss at step  3300 :  4.119481885433197\n",
      "Average loss at step  3400 :  4.092122325897217\n",
      "Average loss at step  3500 :  4.07028666973114\n",
      "Average loss at step  3600 :  4.040950520038605\n",
      "Average loss at step  3700 :  4.047085227966309\n",
      "Average loss at step  3800 :  4.009191422462464\n",
      "Average loss at step  3900 :  3.914623192548752\n",
      "Average loss at step  4000 :  3.9598716938495637\n",
      "Average loss at step  4100 :  3.875674602985382\n",
      "Average loss at step  4200 :  3.8758017551898956\n",
      "Average loss at step  4300 :  3.8743076610565184\n",
      "Average loss at step  4400 :  3.834624147415161\n",
      "Average loss at step  4500 :  3.8375629711151125\n",
      "Average loss at step  4600 :  3.8119361448287963\n",
      "Average loss at step  4700 :  3.8026644134521486\n",
      "Average loss at step  4800 :  3.7615100395679475\n",
      "Average loss at step  4900 :  3.7188664793968202\n",
      "Average loss at step  5000 :  3.7063290321826936\n",
      "Average loss at step  5100 :  3.684121940135956\n",
      "Average loss at step  5200 :  3.6511694049835204\n",
      "Average loss at step  5300 :  3.705748426914215\n",
      "Average loss at step  5400 :  3.6368310046195984\n",
      "Average loss at step  5500 :  3.6354214715957642\n",
      "Average loss at step  5600 :  3.6279373931884766\n",
      "Average loss at step  5700 :  3.612780110836029\n",
      "Average loss at step  5800 :  3.5394874942302703\n",
      "Average loss at step  5900 :  3.5500560653209687\n",
      "Average loss at step  6000 :  3.52988538980484\n",
      "Average loss at step  6100 :  3.5112762236595154\n",
      "Average loss at step  6200 :  3.4907544112205504\n",
      "Average loss at step  6300 :  3.513233859539032\n",
      "Average loss at step  6400 :  3.4665231704711914\n",
      "Average loss at step  6500 :  3.4650684714317324\n",
      "Average loss at step  6600 :  3.4858889961242676\n",
      "Average loss at step  6700 :  3.3995097136497496\n",
      "Average loss at step  6800 :  3.398036959171295\n",
      "Average loss at step  6900 :  3.3984363579750063\n",
      "Average loss at step  7000 :  3.3604424262046813\n",
      "Average loss at step  7100 :  3.361655194759369\n",
      "Average loss at step  7200 :  3.3561809039115906\n",
      "Average loss at step  7300 :  3.341532208919525\n",
      "Average loss at step  7400 :  3.3336059284210204\n",
      "Average loss at step  7500 :  3.304801554679871\n",
      "Average loss at step  7600 :  3.356631824970245\n",
      "Average loss at step  7700 :  3.2491048216819762\n",
      "Average loss at step  7800 :  3.2418831527233123\n",
      "Average loss at step  7900 :  3.277294443845749\n",
      "Average loss at step  8000 :  3.2269421553611757\n",
      "Average loss at step  8100 :  3.2410484528541565\n",
      "Average loss at step  8200 :  3.2273339104652403\n",
      "Average loss at step  8300 :  3.201062197685242\n",
      "Average loss at step  8400 :  3.2194675159454347\n",
      "Average loss at step  8500 :  3.192337865829468\n",
      "Average loss at step  8600 :  3.1692879724502565\n",
      "Average loss at step  8700 :  3.164389338493347\n",
      "Average loss at step  8800 :  3.1163402819633483\n",
      "Average loss at step  8900 :  3.1787434136867523\n",
      "Average loss at step  9000 :  3.1231190586090087\n",
      "Average loss at step  9100 :  3.0988150358200075\n",
      "Average loss at step  9200 :  3.123881416320801\n",
      "Average loss at step  9300 :  3.089883062839508\n",
      "Average loss at step  9400 :  3.106976025104523\n",
      "Average loss at step  9500 :  3.0934672570228576\n",
      "Average loss at step  9600 :  3.02613420009613\n",
      "Average loss at step  9700 :  3.0389974319934847\n",
      "Average loss at step  9800 :  3.0204282641410827\n",
      "Average loss at step  9900 :  3.049004809856415\n",
      "Average loss at step  10000 :  3.0212723231315612\n"
     ]
    }
   ],
   "source": [
    "for i in range(FLAGS.num_step):\n",
    "\n",
    "    step = last_step + i + 1\n",
    "    batch_inputs, batch_labels = data.create_next_batch(batch_size, FLAGS.num_skips, FLAGS.skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    _, loss_val = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        average_loss /= 100\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0\n",
    "        saver.save(sess, FLAGS.log_dir+'my_model.ckpt', step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
